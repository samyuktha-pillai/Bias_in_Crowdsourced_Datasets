# Measuring Bias in Crowdsourced Datasets using PMI

*A hands-on project exploring how stereotypes emerge in crowdsourced text datasets.*

---

## üìå **Project Overview**

Crowdsourced datasets‚Äîsuch as those collected from Mechanical Turk or open community platforms‚Äîoften contain subtle but harmful biases against social groups. These biases propagate into models trained on these datasets, influencing predictions and downstream AI behavior.

This project analyzes **how stereotypes form in crowdsourced text data** using **Pointwise Mutual Information (PMI)** as a measure of association strength between:

* **Target groups** (e.g., gender, race, professions)
* **Descriptive terms or attributes** (e.g., "violent", ‚Äúintelligent‚Äù, ‚Äúemotional‚Äù, etc.)

The notebook walks through loading the dataset, preprocessing text, computing PMI, and interpreting which group‚Äìattribute pairings reveal potential bias.

---

## üéØ **Objectives**

* Understand how linguistic bias is embedded in text datasets.
* Apply **PMI** to quantify association strength between groups and attributes.
* Identify **positive and negative stereotypes** emerging from crowdsourced responses.
* Visualize bias patterns using tables and plots.
* Discuss implications for dataset quality and ethical AI development.

---

## üß† **What is PMI?**

**Pointwise Mutual Information (PMI)** measures how much more frequently two words appear together than expected by chance.

[
PMI(x, y) = \log \frac{P(x,y)}{P(x)P(y)}
]

* **PMI > 0** ‚Üí occurs together more than expected ‚Üí potential stereotype
* **PMI < 0** ‚Üí occurs together less than expected
* **PMI = 0** ‚Üí independent
* Higher absolute PMI values reflect stronger associations (positive or negative)

---

## üìÇ **Project Structure**

```
‚îú‚îÄ‚îÄ biasInCrowdsourcedDataset_v2.ipynb   # Main notebook
‚îú‚îÄ‚îÄ README.md                            # Project documentation
‚îî‚îÄ‚îÄ requirements.txt                     # Python dependencies (if added)
```

---

## *Dataset (Not Included Due to Size)*

This project uses the *SNLI (Stanford Natural Language Inference) dataset* combined with *identity attribute labels* for bias analysis.
Due to its large size, the dataset is *not included* in this repository.

### üîó How to Download the Dataset

1. *Download SNLI* from the official source:
   [https://nlp.stanford.edu/projects/snli/](https://nlp.stanford.edu/projects/snli/)

2. *Identity labels* can be obtained from the same research sources used for bias studies, such as the *CrowS-Pairs* or *Bias-in-Bios* datasets (depending on which one you used).
   If you used a specific label file, mention it like this:

   > Identity labels file (e.g., identity_labels.json or bias_annotations.csv) should be placed in the data/ directory after download.
---

## ‚öôÔ∏è **Key Steps in the Notebook**

1. **Load and inspect the dataset**

   * Review structure, column names, text fields.
2. **Preprocess text**

   * Tokenization
   * Lowercasing
   * Stopword removal
3. **Define target groups and attribute lists**
4. **Compute PMI values**

   * Frequency counts
   * Joint probabilities
   * PMI matrix or table
5. **Visualize results**

   * Heatmaps
   * Bar graphs
6. **Interpret bias patterns**

   * Which groups receive disproportionately negative/positive attributes?
   * Are stereotypes amplified in crowdsourced text?
7. **Discussion on ethical implications**

   * Dataset curation
   * Model impact
   * Mitigation strategies

---

## üìä **Example Outputs**

* PMI heatmaps showing associations
* Top-k stereotypical word associations
* Group-wise comparison charts
* Tables listing strongest positive/negative PMI values

---

## üõ†Ô∏è **Tech Stack**

* **Python**
* pandas
* numpy
* nltk / spaCy
* matplotlib / seaborn
* scikit-learn
* Jupyter Notebook

---

## üß© **How to Run**

1. Clone the repo:

   ```bash
   git clone https://github.com/samyuktha-pillai/Bias_in_Crowdsourced_Datasets.git
   cd yourrepo
   ```
2. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```
3. Open the notebook:

   ```bash
   jupyter notebook biasInCrowdsourcedDataset_v2.ipynb
   ```

---

## ‚ú® **Key Insights**

* Crowdsourced datasets can unintentionally encode **stereotypical associations**.
* PMI is a simple but powerful tool for **quantifying linguistic bias**.
* Bias detection is essential before using datasets for training ML models.
* Understanding these patterns helps move toward **fairer and more responsible AI**.

---

## üìò **Future Work**

* Extend analysis to contextual embeddings (e.g., BERT).
* Use PMI variants: normalized PMI (nPMI).
* Run significance testing on associations.
* Apply debiasing methods to reduce harmful correlations.

---

## ü§ù **Contributions**

Pull requests are welcome!
Feel free to open an issue for improvements, bugs, or feature suggestions.

---
